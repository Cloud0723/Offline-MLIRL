# halfcheetah experiments debug
# medium: 11 0.96 37
args:
    env_name: halfcheetah-medium-replay-v2 # d4rl mixed, supposedly the best for MOPO
    reward_head: True
    logvar_head: True
    states: 'uniform'
    steps_k: 37
    reward_steps: 200
    num_rollouts_per_step: 50
    policy_update_steps: 1000
    train_policy_every: 100
    train_val_ratio: 0.2
    real_sample_ratio: 0.05
    model_train_freq: 1000
    max_timesteps: 10000000
    n_eval_rollouts: 10
    num_models: 11
    num_elites: 5
    d4rl: True
    model_retain_epochs: 5
    mopo: True
    mopo_lam: 0.96
    offline_epochs: 1000
    augment_offline_data: False
    mopo_penalty_type: ensemble_var
    load_model_dir: /home/luoqijun/code/IRL_Code/MBIRL/rethinking-code-supp/data/model_halfcheetah/halfcheetah_mixed_v2_seed0/checkpoints/model_saved_weights/Model_halfcheetah-medium-replay-v2_seed0_2022_11_20_01-52-59
    save_policy: True
    save_model: True
    train_memory: 2000000
    val_memory: 500000
    transfer: True