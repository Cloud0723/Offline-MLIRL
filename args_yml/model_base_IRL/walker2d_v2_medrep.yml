# N, lambda, h = 12 0.99 37  penalty_teype: Ensemble Std
# environment: walker2d-medium-expert-v0

args:
    env_name: walker2d-medium-replay-v2 # d4rl mixed, supposedly the best for MOPO
    reward_head: True
    logvar_head: True
    states: 'uniform'
    steps_k: 37
    reward_steps: 200
    num_rollouts_per_step: 50
    policy_update_steps: 1000
    train_policy_every: 100
    train_val_ratio: 0.2
    real_sample_ratio: 0.05
    model_train_freq: 1000
    max_timesteps: 10000000
    n_eval_rollouts: 10
    num_models: 13
    num_elites: 5
    d4rl: True
    model_retain_epochs: 5
    mopo: True
    mopo_lam: 0.99
    mopo_penalty_type: ensemble_std
    #tune_mopo_lam: False
    min_model_epochs: 350
    offline_epochs: 2000
    save_model: True
    save_policy: True
    load_model_dir: /home/luoqijun/code/IRL_Code/MBIRL/rethinking-code-supp/data/model_walker2d/walker2d-medreplay/checkpoints/model_saved_weights/Model_walker2d-medium-replay-v2_seed0_2022_11_28_14-28-24
    train_memory: 1000000
    val_memory: 500000
    transfer: True